\documentclass[a4paper,UTF8]{ctexart}
%\usepackage{ctex}使用该句会报redefine的bug
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\theoremstyle{definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\usepackage{multirow}
\usepackage{url}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}




%code
%\usepackage[utf8]{inputenc}
 
\usepackage{listings}
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}
%code











\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Procedure:}}
\renewcommand\refname{参考文献}

%--

%--
\begin{document}
\title{丰富的特征结构，用于精确的对象检测和语义分割}
\author{Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik UC Berkeley}
\maketitle
\begin{abstract}
对象检测性能，在标准(canonical) PASCAL VOC 数据集上测量，在最近几年一直稳定(plateau）.最优性能的方法是复杂集成系统(complex ensemble system)典型地联合(combine)多个(multiple）低层次的图像特征和高层次的内容。在这篇paper中，我们提出一个简单并且可扩展的检测算法，提升了平均精度mAP(mean average precision)30个百分点，相对于之前在VOC 2012 中最好的结果-获得 53.3\%的mAP.我们的方法联合两个关键的见解(key insights):(1)人们可以将高容量(high-capacity)卷积神经网络（CNN）应用于自下而上的区域候选以便定位和分割对象.以及(2) 当有标记训练数据稀缺(scarce)时,一个辅助(auxiliary)任务的监督预训练，紧跟着一个特定领域的微调(domain-specific fine-tuning),这种训练方式,产生了(yield)一个显著的性能提升(signficant performance boost），因为我们结合了区域候选和CNN,所以我们叫我们的方法R-CNN:结合CNN特征的区域.整个系统的源代码提供地址是
\end{abstract}

\section{介绍}
特征很重要(feature matter).上个世纪在不同视觉识别任务上的进展颇(considerably)基于SIFT和HOG的使用。但是如果我们看在经典(canonical）视觉识别任务--PASCAL VOC物体检测,上的性能,被广泛承认的是，在2010-2012间进展缓慢，，只有通过建立集成系统和使用成功方法的小变体(minor variant)获得的小收益(small gains)

SIFT 和HOG 是块定向直方图(blockwise orientation histogram),一种我们可以与V1层中的细胞粗糙(roughly)地联系起来的特征表示.V1层的细胞是灵长类(primate)视觉通路(visual pathway)的第一皮质区域(first cortical area).但我们也知道，识别发生在下游的几个阶段，这表明可能存在分层的，多阶段(multi-stage)的计算特征，这些特征对于视觉识别甚至更具信息性(more informative)

Fukushima的"神经认知机(neocognitron)",一个生物启发的分层和移位不变性的模式识别的模型，是对这种过程的早期尝试(an early attempt at just such a process).神经认知机，但是缺少(lack)一个监督训练算法。建立在Rumelhart等人的基础上(building on)，LeCun等人 表明通过反向传播的随机梯度下降对于训练卷积神经网络（CNNs),即扩展了神经认知机的一类模型，是有效的。

20世纪90年代，CNN被大量使用(saw heavy use)，但随后随着支持向量机的兴起(with the rise of)，它已经不再流行(fell out of fashion)。在2012年，Krizhevsky等人重新激发了(rekindle)CNN的兴趣，通过在ImageNet大规模视觉识别挑战（ILSVRC）上显示出更高的图像分类准确性。他们的成功是通过在120万张标签图像上训练大型CNN，以及LeCun CNN上的一些修改(twists)（例如，max（x; 0）纠正(rectifying)非线性和“dropout”正则化)

ILSVRC 2012研讨会期间，ImageNet结果的重要性引起了激烈的争论(was vigorously debated).中心问题可以归结为以下几点(be distilled to the following)：在多大程度上(to what extent)ImageNet上的CNN分类结果可以归纳为(generalize to)PASCAL VOC挑战中的对象检测结果.我们通过建立图像分裂与对象检测之间的桥梁来回答这个问题(We answer this question by bridging the gap between image classification and object detection).本文首次证明，与基于简单HOG特征的系统相比，CNN可以显着提高PASCAL VOC上的目标检测性能。为了达到这个结果(achieve this result），我们集中在两个问题:使用深度网络对物体进行定位和通过只有少量的注释检测数据训练高容量模型

不像图像分类,检测需要在图像中定位对象(可能很多 likely many)的位置.一种方法将定位定义为回归问题(One approach frames localization as a regression problem).但是来自Szegedy的工作，与我们自己的工作同时(current with our own），表明(indicate）这个策略有可能实际中表现不好(not fare well).(他们报告了VOC 2007中30.5\%的mAP,与我们方法获得的58.5\%相比).一个替换的方法(an alternative)是建立一个滑动窗口检测器(sliding-windows detector).CNNs已经用于这种方式至少二十年(two decades)，通常用于受限的物体类别(constrained object categories)，如人脸和行人(pedestrians)。为了保持高空间分辨率(high spatial resolution)，这些CNN通常只有两个卷积层和池化层。我们也考虑过采用(adopt）滑动窗口的方法。但是位于我们网络顶层的单元units high up in our network),具有五个卷积层，在输入图像中有非常大的感受野(receptive field)(195*195 pixels)和步长(stride)(32*32 pixels),使得在滑动窗口范式(paradigm)下精确的定位成为一个开放的技术难题

相反(instead),我们通过在"使用区域识别"范式内操作，来解决CNN定位问题,这个范式被成功的用于目标检测和语义分割.
在测试时间，我们的方法为输入图像生成大约2000个与分类无关(category-independent)的区域候选，使用CNN从每个候选中提取(extract)固定长度的特征向量，然后使用特定类别的线性SVM对每个区域进行分类。我们使用一个简单的技术(放射图像变形 affine image warping),来计算每个区域候选对应的固定大小的CNN输入，而不考虑区域的形状(regardless of).图1概述了(overview)我们的方法并突出了(highlights)我们的一些结果。由于我们的系统将区域候选与CNN结合在一起，因此我们将该方法称为R-CNN(dub the method R-CNN)：Regions with CNN features

在检测中面临的第二个挑战是带标签数据是稀缺的(scarce),并且目前可用的数量不足以(be insufficient)训练一个大型CNN.这个问题的传统解决方案是使用无监督预训练,然后紧跟着监督微调.本文的第二个原则贡献是显示对大数据辅助数据集（ILSVRC）进行有监督的预训练，然后对小数据集（PASCAL）进行领域特定(domain-specfic)的微调，这是一种数据稀缺时有效的学习高容量CNN的范式.在我们的誓言中，检测微调提高了mAP 8个百分点。经过微调后，我们的系统在VOC 2010上达到54\% 的MAP，而高度调整(high-tuned)的基于HOG的deformable part model（DPM）则为33\% .我们也向读者指出同时期的(contenporaneous)Donahue等人的工作。 [11]，他们证明Krizhevsky的CNN可以作为黑箱特征提取器使用（不用微调），在几种识别任务中表现出色，包括场景分类，细粒度(fine-grained)的子分类,领域适应(domain adaptation).

我们的系统也非常有效(efficient),唯一的class-specific 计算是相当小的矩阵向量相乘和贪婪的非最大化抑制(non-maximum suppression).这种计算特性遵循所有类别共享的特征，并且也比先前使用的区域特征低两个数量级。

了解我们方法的失效模式(failure modes)对于改进方法也很关键(critical)，因此我们报告了Hoiem等人的检测分析工具的结果。作为这种分析的直接后果(immediate consequence)，我们证明(demonstrate)了一种简单的边界框回归方法显着减少了错误定位，这是主要的错误模式(dominate error mode)。在开发技术细节之前，我们注意到，由于R-CNN在区域上运作，很自然地将它扩展到语义分割的任务。只需稍作修改(minor modifications)，我们还可以在PASCAL VOC分割任务上取得具有竞争力(competitive)的结果，VOC 2011测试集的平均分割准确率为47.9\%
\section{使用R-CNN做对象检测}
我们的目标检测系统包括三个模块.第一个生成(generate)类别无关的区域候选。这些候选定义了我们检测器可用的候选检测集.第二个模块是一个大型卷积神经网络从每个区域提取固定长度的特征向量。第三个模块是一个特定类别的线性SVMs的集合。在这个部分，我们呈现每个模块的设计决策，描述他们的测试时间使用情况(，详细了解他们的参数如何学习，并在PASCAL VOC 2010-12上展示结果
\subsection{模块设计}
\paragraph{区域候选.} 最近的各种论文提供生成类别独立的区域候选的方法。例子包括:objectness,selective search,category-independent object proposals,constrained parametric min-cuts(CPMC),multi-scale combinatorial grouping,和Ciresfort等人 [6]，他们通过将CNN应用于定期间隔的方形作物来检测有丝分裂细胞，这是区域候选的特例(special case of region proposals),既然R-CNN 对特殊的区域候选方法是不可知的(agnostic),我们使用selective search来确保与之前(prior)检测工作的可控的(controlled)比较
\paragraph{特征提取.} 我们从每个区域候选提取4096维的特征向量，通过使用Krizhevsky提出的CNN的Caffe方案。通过在五个卷积层和两个全连接层中前向传播(forward propagating)一个mean-subtracted 227*227RGB图像，来计算特征。我们建议读者参考[22,23]来得到更多的网络架构细节。

为了计算一个区域候选的特征，我们首先需要把那个区域的图像数据转化成与CNN兼容的形式(CNN架构需要固定的227*227像素大小).在诸多我们任意形状的区域的可能的变换中，我们选择(opt for)最简单的.不管候选区域的大小或长宽比(aspect ratio)，我们将其周围的一个紧密包围盒中的所有像素扭曲(warp)到所需的大小。Prior to warping, we dilate the tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original box (we use p = 16). 图二显示了变形后的训练区域的随机样本。补充材料讨论了变形的替代方法(The supplementary material discusses alternatives) to warping)

\subsection{测试时间内的检测}
在测试时间内,我们在测试图片上运行了select search,提取了差不多2000区域候选(我们在所有的实验中选择了select search的"fast mode").我们对每个候选进行变形，并且在CNN中前向传播它，以便从所需的(desired=wanted)层次中读取特征.然后对每一个类，我们使用在那个类上训练出来的SVM给每个被提取的特征向量打分.在给定一个图像中所有被打分的区域后，我们应用一个贪婪非最大化抑制(对每个类独立地使用),如果一个区域的与较高得分的选定区域交叠的交集(intersection-over-union)（IoU）大于学习阈值，则拒绝该区域

\paragraph{运行时间分析.}两个性质使得检测非常高效.首先，CNN的所有参数被所有的分类共享。其次，被CNN计算得到的特征向量是低维的，当与其他的一般方法比较，比如说spatial pyramids with bag-of-visual-word encodings(使用视觉词袋编码的空间金字塔).例如，在UVA检测系统中使用的特征比我们的要大两个数量级(two orders of magnitude larger than)(360k vs 4k-dimensional)

这种共享的结果是花在计算区域候选和它的特征的时间被分摊(amortized)到所有的类别上.唯一的特定类别的计算是，特征和SVM权重和非最大化抑制之间的点积运算。实际上,一个图片的所有的点积运算被批量为一个 矩阵相乘运算.特征矩阵是2000*4096,以及SVM权重矩阵是4096*N.

这个分析表明，R-CNN可以扩展到(scale to)数千个对象类别，而并不需要求助于(resorting to)近似技术，比如hashing.甚至即使有100k个类别，矩阵相乘只需要10秒，在一个现代多处理器CPU上。这种高效并不只是使用区域候选和共享的特征的结果。UVA系统，因为它的高维特征，会慢两个数量级，并且需要134GB的内存来存储100k个线性预测器，与我们的低维特征只需要1.5GB相比.

将R-CNN与Dean等人的最近的工作进行对比(contrast to)也是非常有趣的。他们的工作主要集中在使用DPMs和hashing来做scalable detection.他们报告了在introducing 10k distractor classes的情况下，VOC 2007每个图片运行5分钟，获得了16\%的mAP.没有我们的方法，10k的检测器可以在CPU上运行差不多1分钟，并且因为没有采取近似手段，mAP会保持(remain at)在59\%(section 3.2)
\subsection{训练}
\paragraph{监督预训练.}我们在带有图像级注释（image annotations)（即没有边界框标签）的大型辅助数据集（ILSVRC 2012）上有区别地(discriminatively)预先训练了CNN。使用开源代码Caffe CNN库来做预训练.简而言之(in brief)，我们的CNN几乎与Krizhevsky等人的表现相匹配,获得ILSVRC 2012验证集的比第一名高2.2百分点的错误率。 这种差异(discrepancy)是由于训练过程的简化

\paragraph{专门领域的微调.}为了将我们的CNN适配到新的任务(检测),新的定义域(变形的VOC 窗口),我们继续使用随机梯度下降(SGD)训练CNN的参数，只使用VOC的变形后的区域候选作为输入。除了(aside from)用随机初始化的21路分类层（对于20个VOC类加上背景）替换CNN的特定于ImageNet的1000路分类层之外，CNN体系结构不变。我们将与实际的框重叠区域$\geq 0.5$IoU的区域候选作为该框所在类别的正例，其余的做反例。我们以0.001的学习率开始SGD(初始预训练率的1/10th),这允许在进行微调取得进展的同时，不破坏(clobber)初始化.在每一次SGD的迭代中，我们平均采样32正例窗口(在所有的类别中)和96个背景窗口来构建一个138大小的mini-batch.我们偏向(bias)正例窗口采样，因为它们与背景相比极其罕见。

\paragraph{对象类别分类器.}考虑训练一个二分类器(binary classifier)来检测车辆.很显然，一个紧紧围绕(tightly enclosing)汽车的图像区域应该是一个正例.类似的(similarly),很显然一个跟车没关系的背景区域应该是一个反例.不怎么明显的是怎么给一个部分与车重叠的区域贴标签。我们通过一个IoU重叠阈值来解决这个问题,在这个阈值之下区域被认定为反例。重叠阈值,0.3,是在验证集上网格搜索{0,0.1,....,0.5}得到的。我们发现仔细选择这个阈值是非常重要的。把它设为0.5,mAP下降了5个点，相同的，把它设为0，mAP下降了4个点。Positive examples are defined simply to be the ground-truth bounding boxes for each class

一旦特征被提取，并且训练集标签被确定下来，我们对每个类优化一个线性SVM。因为训练数据太大，不能放进内存中，我们采用standard hard negative mining method.hard negative mining 收敛(converge)非常快，而且在实际中,一遍通过所有图片后，mAP就停止增长。

在补充材料中，我们讨论了为什么在fine-tuning阶段和(versus)SVM training阶段，正例和反例在这两个阶段中定义不同。我们也讨论了为什么训练检测分类器而不是简单地使用fine-tuned CNN的最后一层的输出是必要的。
\section{在PASCAL VOC 2010-12 数据集上的结果}
\end{document}